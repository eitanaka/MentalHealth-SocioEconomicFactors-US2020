---
title: "Lifecycle and mental health"
author: "Ishani Makwana, Henry Hirsch and Ei Tanaka"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```

```{r}
# Import Libraries
rm(list=ls())
library(readr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(ezids)
library(car)
library(rpart)
library(rpart.plot)
library(rattle)
```

```{r init}
url <- 'https://raw.githubusercontent.com/eitanaka/DATS6101_Final_Project_Team2/main/data_set/geo_socio_health_df.csv'
master_df<- read_csv(url)
```

## Abstract
Need edited

An abstract is an overview of a research project that seeks to explore the relationship between mental health and various socio-economic and lifestyle factors, as well as the impact of COVID-19 in the United States.The aim is to answer the question of whether there is a significant influence on mental health due to these factors in the US, and data will be analyzed at the census tract level using 11 different datasets. The abstract highlights the main objectives, methods, results, and conclusions of the research, while the introduction sets the stage by discussing the relevance of the research question and the data sets used.This question is relevant to the project's aim and can guide the data analysis and interpretation.

## Introduction
In project 1, we investigated how health risk behaviors are associated with health outcomes and health status.We have 11 different datasets that we have made used of. We have tried importing all the datasets from the google drive where all datasets has been uploaded.Now, we are trying to examine whether there is any significant influence on mental health due to lifestyle, socio-economic factors, and covid-19 in the United States. Census tracts will be used in calculating and analyzing all the different datasets. This research which is going to be very interesting to look at identifying what factors have significantly impacted mental health and well-being in the United States in 2020. 


### Data sets
Need edited (Describe about 4 data set and how we create)
We have used of 10 different datasets and the one which we are going to use from the previous Data Science project1. 
<!-- CDC_Place : We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables including "Year","StateAbbr","StateDesc","CountyName","CountyFIPS","LocationName","Data_Value","TotalPopulation","MeasureId" -->

<!-- Census Tract Relationship 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables -->
               
<!-- Commute Time 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables-->
<!-- Econ Impact Index 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables-->
<!-- Educational Attainment 2020:We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables-->
<!-- Employment Status 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables -->
<!-- Marital Status 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables-->
<!-- Median Income 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables-->
<!-- Planning Database with 2010 Census and 2014 â€“ 2018 American Community SurveyData: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables -->
<!-- Population 2020: We imported the dataset from the shared drive and have cleaned up dataset and made used of some variables -->

### Variables of interest
# Here are all the variables used in the project.
<!-- CountyFIPS: a numeric column containing the FIPS code for each county. -->
<!-- GEOID: a numeric column containing the unique  Geographical ID for each county. -->
<!-- StateAbbr: a character column containing the abbreviation for the state where each county is located. -->
<!-- CountyName: a character column containing the name of each county. -->
<!-- BINGE: a numeric columns representing percentages of population in each county that reported binge drinking -->
<!-- DEPRESSION:a numeric columns representing percentages of population in each county that reported depression -->
<!-- LPA:a numeric columns representing percentages of population in each county that reported doing less physical activity -->
<!-- MHLTH:a numeric columns representing percentages of population in each county that reported poor mental health -->
<!-- SLEEP:a numeric columns representing percentages of population in each county that reported insufficient sleep. -->
<!-- MT_Total: Total marital status percentage. -->
<!-- MT_Never Married:a numeric columns representing percentages of adults in each county who were categorized as never married -->
<!-- MT_Now married:a numeric columns representing percentages of adults in each county who were categorized as now married. -->
<!-- MT_Divorces:a numeric columns representing percentages of adults in each county who were categorized as got divorces. -->
<!-- MT_Separated:a numeric columns representing percentages of adults in each county who were categorized as couple who separated. -->
<!-- MT_Widowed:a numeric columns representing percentages of adults in each county who were categorized as widowed. -->
<!-- EA_Total: Total educational population percentage. -->
<!-- EA_Less than high school graduate:a numeric columns representing percentages of adults in each county who were categorized as studied less than high school -->
<!-- EA_High school graduate: a numeric columns representing percentages of adults in each county who were categorized who are high school graduate. -->
<!-- EA_college or associate's degree:a numeric columns representing percentages of adults in each county who were categorized who did college or associate's degree -->
<!-- EA_Bachelor's degree: a numeric columns representing percentages of adults in each county who were categorized as completed bachelor's degree -->
<!-- EA_Graduate or professional degree:a numeric columns representing percentages of adults in each county who were categorized as having a certain educational graduate or professional degree. -->
<!-- MI_Estimate:a numeric column representing the estimated median household income in each county. -->
<!-- Total Population:a numeric column representing the total population of each county. -->
<!-- CT_Total:a numeric column representing the percentages of the population in each county that take total commute time in percentage. -->
<!-- CT_<10:a numeric column representing the percentages of the population in each county that take commute time less than 10min -->
<!-- CT_10-14:a numeric column representing the percentages of the population in each county that take commute time between 10 -14min -->
<!-- CT_20-24:a numeric column representing the percentages of the population in each county that take commute time between 20 -24min -->
<!-- CT_25-29:a numeric column representing the percentages of the population in each county that take commute time between 25 -29min -->
<!-- CT_30-34:a numeric column representing the percentages of the population in each county that take commute time between 30 -34min -->
<!-- CT_35-44:a numeric column representing the percentages of the population in each county that take commute time between 35 -44min -->
<!-- CT_45-54:a numeric column representing the percentages of the population in each county that take commute time between 45 -54min -->
<!-- CT_55-59:a numeric column representing the percentages of the population in each county that take commute time between 55 -59min -->
<!-- CT_>60:a numeric column representing the percentages of the population in each county that take commute time more than 60 minutes -->

<!-- ES_Total: This variable represents the percentage of the total population that is employed or actively seeking employment. -->
<!-- ES_Total_labor_force: This variable represents the percentage of the total population that is either employed or actively seeking employment out of the total labor force. -->
<!-- ES_Civilian_labor_force: This variable represents the percentage of the total population that is either employed or actively seeking employment within the civilian labor force. -->
<!-- ES_Civilian_labor_force_employed: This variable represents the percentage of the total civilian labor force that is currently employed. -->
<!-- ES_Civilian_labor_force_unemployed: This variable represents the percentage of the total civilian labor force that is currently unemployed and actively seeking employment. -->
<!-- ES_Armed_Forces: This variable represents the percentage of the total population that is serving in the armed forces. -->
<!-- ES_Not_in_labor_force: This variable represents the percentage of the total population that is not part of the labor force, either due to being retired, not seeking employment, or being unable to work. -->
<!-- LAND_AREA: This variable represents the total land area in square kilometers. -->
<!-- URBANIZED_AREA_POP_CEN_2010: This variable represents the population in urbanized areas as of the 2010 Census. -->
<!-- URBAN_CLUSTER_POP_CEN_2010: This variable represents the population in urban clusters as of the 2010 Census. -->
<!-- RURAL_POP_CEN_2010: This variable represents the rural population as of the 2010 Census. -->
<!-- pct_URBANIZED_AREA_POP_CEN_2010: This variable represents the percentage of the total population living in urbanized areas as of the 2010 Census. -->
<!-- pct_URBAN_CLUSTER_POP_CEN_2010: This variable represents the percentage of the total population living in urban clusters as of the 2010 Census. -->
<!-- pct_RURAL_POP_CEN_2010: This variable represents the percentage of the total population living in rural areas as of the 2010 Census. -->
<!-- index_apr20: This variable represents an index value for the area in question as of April 2020. -->
<!-- pop.den: This variable represents the population density of the area in question. -->

### Initial SMART Question
Need edited
1) Why does the United States have a large number of people with depression and mental health issues? Is the lifestyle, socio-economic factors, and covid-19 in the States leading people towards poor mental health?

2) How can we expect predictors to contribute to increased depression and poor mental health?

## EDA
Need edited
Let us work on some data manipulation and visualization tasks by renaming some columns in the dataframe and the second part of your code creates two histograms. The first histogram (dep_hist) shows the distribution of tract-level depression rates, and the second histogram (mhlth_hist) shows the distribution of tract-level poor mental health rates.
```{r}
# Checking normality of dependent variables.
# To rename some columns in a data frame to make them more readable and easier to work with.This is done by using the "colnames" function to first select the column names that match the original names, and then assigning new names.
colnames(master_df)[colnames(master_df) == "MT_Never Married"] <- "MT_never_married"
colnames(master_df)[colnames(master_df) == "MT_Now married"] <- "MT_now_married"
colnames(master_df)[colnames(master_df) == "Total Population"] <- "total_population"
colnames(master_df)[colnames(master_df) == "EA_Less than high school graduate"] <- "EA_Less_than_high_school_graduate"
colnames(master_df)[colnames(master_df) == "EA_High school graduate"] <- "EA_High_school_graduate"
colnames(master_df)[colnames(master_df) == "EA_college or associate's degree"] <- "EA_college_or_associates_degree"
colnames(master_df)[colnames(master_df) == "EA_Bachelor's degree"] <- "EA_Bachelors_degree"
colnames(master_df)[colnames(master_df) == "EA_Graduate or professional degree"] <- "EA_Graduate_or_professional_degree"

# Histograms
# Two histograms: one for the distribution of tract-level depression rates and one for the distribution of tract-level poor mental health rates.
dep_hist <- hist(master_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
dep_hist
mhlth_hist <- hist(master_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
mhlth_hist
```
Looking at the histograms, it appears that both variables are approximately normally distributed, although the distribution of tract-level depression rates is skewed to the right, with a larger number of observations in the higher bins. The distribution of tract-level poor mental health rates also appears skewed to the right, but with a more gradual decrease in the number of observations in the higher bins compared to the depression rates histogram.This information is important when selecting statistical methods to analyze these variables, as many methods assume normality. Additionally, the histograms provide a visual representation of the distribution of the variables, which can help in identifying potential outliers or unusual patterns.

```{r}
# QQ plots for the distribution of tract-level depression rates and tract-level poor mental health rates.

dep_qq <- qqnorm(master_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
dep_qq
mhlth_qq <- qqnorm(master_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
mhlth_qq
```
A QQ plot compares the quantiles of a dataset to the quantiles of a theoretical distribution (in this case, the normal distribution). If the points on the QQ plot are close to a straight line, it indicates that the data are normally distributed.
In this case, the QQ plot for the depression rates appears to deviate from a straight line towards the tails, indicating that the distribution is not perfectly normal. The QQ plot for the poor mental health rates appears to deviate from a straight line even more than the depression rates, indicating that the distribution is even less normal.

# create data frame of only numeric values
 We will create a new data frame called num_df that only contains the numeric columns of the original master_df, that can be analyzed using various statistical tools.
```{r}
cols_to_remove <- c("...1", "CountyFIPS", "GEOID", "StateAbbr", "CountyName", "MT_Total", "EA_Total", "total_population", "CT_Total", "ES_Total", "LAND_AREA")
num_df <- master_df[ , !names(master_df) %in% cols_to_remove]

# hist_list <- lapply(names(num_df), function(col) {
#   ggplot(num_df, aes_string(x = col)) +
#     geom_histogram() +
#     ggtitle(col)
# })
# hist_list
```

### Basic EDA
Need edited
```{r}


```

#### Handle Null Values
```{r}
master_df <- na.omit(master_df)
colSums(is.na(master_df[6:length(master_df)]))
```
The output will be a numeric vector with the number of missing values in each column. If all values are 0, then there are no missing values in the selected columns.

#### Handle Outlier
```{r}
# Create a function to plot boxplot with distribution lines
# Function to create boxplot with mean and standard deviation
# Load the necessary libraries
library(ggplot2)
library(gridExtra)

# Create a function to plot boxplot with distribution lines
plot_boxplot <- function(x) {
  # Calculate mean and standard deviation
  mean_x <- mean(x, na.rm = TRUE)
  sd_x <- sd(x, na.rm = TRUE)
  
  # Create the plot
  ggplot(data = data.frame(x), aes(x = x)) +
    geom_boxplot() +
    geom_vline(xintercept = mean_x, color = "blue", size = 1) +
    geom_vline(xintercept = mean_x - sd_x, color = "red", linetype = "dashed", size = 1) +
    geom_vline(xintercept = mean_x + sd_x, color = "red", linetype = "dashed", size = 1) +
    labs(x = names(x), y = "Value") +
    theme_bw() +
    theme(plot.title = element_text(size = 12, hjust = 0.5),
          axis.title.x = element_text(size = 10),
          axis.title.y = element_text(size = 10),
          axis.text = element_text(size = 8),
          axis.line = element_line(colour = "black"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none")
}

# Filter the numeric variables
numeric_vars <- sapply(master_df, is.numeric)
numeric_df <- master_df[, numeric_vars]

# Create boxplots for all numeric variables
par(mfrow=c(2,4)) # to display plots in a grid of 4x4
for (i in 1:ncol(numeric_df)) {
  boxplot(numeric_df[,i], main=names(numeric_df)[i])
}
```
The code creates a function called plot_boxplot that plots a boxplot with mean and standard deviation lines using ggplot. It then filters the numeric variables from the master_df dataframe and creates a new dataframe called numeric_df.Finally, it creates boxplots for all numeric variables in numeric_df using a for loop. This code allows us to quickly visualize the distribution of the numeric variables and to identify any outliers or unusual patterns.

## Data Structure
Need edited
It can be useful to get a quick overall structure of the data frame before performing further analyses.
```{r}
str(master_df)
```


## Summary Statictics
Need edited
It can be useful to get a overview  of summary statistics of the data frame before performing further analyses.
```{r}
summary(master_df)
```

### Descriptive Statistics
Need edited 
```{r}
# Create a function to plot histograms with distribution lines
plot_histogram <- function(x) {
  # Calculate mean and standard deviation
  mean_x <- mean(x, na.rm = TRUE)
  sd_x <- sd(x, na.rm = TRUE)
  
  # Create the plot
  ggplot(data = data.frame(x), aes(x = x)) +
    geom_histogram(aes(y = ..density..), binwidth = 1, colour = "black", fill = "white") +
    geom_density(alpha = .2, fill = "#FF6666") +
    geom_vline(xintercept = mean_x, color = "blue", size = 1) +
    geom_vline(xintercept = mean_x - sd_x, color = "red", linetype = "dashed", size = 1) +
    geom_vline(xintercept = mean_x + sd_x, color = "red", linetype = "dashed", size = 1) +
    labs(x = names(x), y = "Density") +
    theme_bw() +
    theme(plot.title = element_text(size = 12, hjust = 0.5),
          axis.title.x = element_text(size = 10),
          axis.title.y = element_text(size = 10),
          axis.text = element_text(size = 8),
          axis.line = element_line(colour = "black"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none")
}

# Filter the numeric variables
numeric_vars <- sapply(master_df, is.numeric)
numeric_df <- master_df[, numeric_vars]

# Create a list of plots for all numeric variables
plots <- lapply(numeric_df, plot_histogram)

plots
```



### Hypothesis Testing
Need edited

### Correlations
```{r}
cor_matrix <- cor(master_df[,6:length(master_df)])

# Create 6 subset for cor_matrix or cor_heatmap
dep_mth_df <- subset(master_df, select = "DEPRESSION")
dep_mth_df <- cbind(dep_mth_df, master_df[,9])

# Data frame containing depression, mhlth, health risk behaviors
dep_mth_hrb <- dep_mth_df
dep_mth_hrb["BINGE"] <- master_df[,6]
dep_mth_hrb["LPA"] <- master_df[,8]
dep_mth_hrb["SLEEP"] <- master_df[,10]

# Data frame containing depression, mhlth, MT
dep_mth_mt <- cbind(dep_mth_df, master_df[12:16])
# Data frame containing depression, mhlth, EA, MI, Pop
dep_mth_ea_mi_pop <- cbind(dep_mth_df, master_df[18:24])
# Data Frame containing depression, mhlth, MI, Pop, CT
dep_mth_ct <- cbind(dep_mth_df, master_df[26:34])
# Data Frame containing depression, mhlth, ES 
dep_mth_es <-cbind(dep_mth_df, master_df[36:41])
# Data Frame containing depression, mhlth, others
dep_mth_geo <- cbind(dep_mth_df, master_df[42:length(master_df)])

# A set of cormatrix
cor_matrix_HRB <- cor(dep_mth_hrb)
cor_matrix_MT_EA <- cor(dep_mth_mt)
cor_matrix_EA_MI_POP <- cor(dep_mth_ea_mi_pop)
cor_matrix_CT <- cor(dep_mth_ct)
cor_matrix_ES <- cor(dep_mth_es)
cor_matrix_GEO <- cor(dep_mth_geo)

# Plot
corrplot(cor_matrix_HRB, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
corrplot(cor_matrix_MT_EA, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
corrplot(cor_matrix_EA_MI_POP, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
corrplot(cor_matrix_CT, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
corrplot(cor_matrix_ES, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
corrplot(cor_matrix_GEO, method = "circle", type = "upper", order = "hclust", addCoef.col = "black", tl.col = "black", tl.srt = 45, cl.pos = "n")
```


#### Variable Selection
Need edited
```{r}
cols_names_depression <- names(which(abs(cor_matrix[2, ]) >= 0.3))
depression_cor_df <- master_df[, c("DEPRESSION", "LPA", "EA_High_school_graduate", "EA_Bachelors_degree", "EA_Graduate_or_professional_degree", "MI_Estimate", "URBANIZED_AREA_POP_CEN_2010", "pct_URBANIZED_AREA_POP_CEN_2010")]

cols_mhlth <- names(which(abs(cor_matrix[4, ]) >= 0.3))
mhlth_cor_df <- master_df[,c("MHLTH", "LPA", "SLEEP", "MT_never_married", "MT_now_married", "MT_Separated", "EA_Less_than_high_school_graduate", "EA_High_school_graduate", "EA_Bachelors_degree", "EA_Graduate_or_professional_degree", "MI_Estimate", "ES_Civilian_labor_force_employed", "ES_Civilian_labor_force_unemployed")]
```

### VIF Test for Multicollinearity
```{r}
calculate_VIF <- function(data, target_col) {
  X <- data[, !colnames(data) %in% target_col]
  vif <- data.frame(
    Feature = colnames(X),
    VIF = apply(X, 2, function(x) vif(lm(x ~ ., data=X)))
  )
  return(vif)
}

depression_VIF <- calculate_VIF(depression_cor_df, "DEPRESSION")
mhlth_VIF <- calculate_VIF(mhlth_cor_df, "MHLTH")

depression_VIF
mhlth_VIF
```

## Model Building
Need edited

### Multiple Linear regression vs. Regression Tree (we must delete this part in the summary paper)
Multiple linear regression and regression tree are two different types of models used for predicting the value of a dependent variable based on one or more independent variables. The main differences between the two models are:

- Model complexity: Multiple linear regression models assume a linear relationship between the dependent and independent variables, and involve estimating a set of coefficients that describe the relationship. In contrast, regression trees involve recursively partitioning the data into subsets based on the independent variables, and fitting a separate model to each subset. This makes regression trees more flexible than linear regression models, but also more complex and harder to interpret.

- Variable selection: Multiple linear regression models typically involve selecting a subset of independent variables that are most predictive of the dependent variable. This is often done using methods such as stepwise regression or regularization. In contrast, regression trees can use all available independent variables in the model, and automatically select the most important variables based on the tree structure.

- Interpretability: Multiple linear regression models provide interpretable coefficients that describe the relationship between the dependent and independent variables. In contrast, regression trees can be harder to interpret, as they involve recursively partitioning the data into subsets based on the independent variables. However, decision trees can be visualized and interpreted to gain insights into the relationships between the independent variables and the dependent variable.

- Performance: The performance of multiple linear regression models depends on the linearity and homoscedasticity assumptions, and may suffer from overfitting or underfitting if the model is too simple or too complex. In contrast, regression trees are more flexible and can capture nonlinear relationships between the dependent and independent variables. However, they may also suffer from overfitting if the tree is too deep or complex.

Overall, multiple linear regression and regression tree are both useful models for predicting the value of a dependent variable based on one or more independent variables. The choice between the two models depends on the specific problem and the data available.

### Multiple Linear Regression
```{r}
# multiple linear regression model for depression
depression_model <- lm(DEPRESSION ~ LPA + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + URBANIZED_AREA_POP_CEN_2010 + pct_URBANIZED_AREA_POP_CEN_2010, data = depression_cor_df)

# multiple linear regression model for poor mental health
mhlth_model <- lm(MHLTH ~ LPA + SLEEP + MT_never_married + MT_now_married + MT_Separated + EA_Less_than_high_school_graduate + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed, data = mhlth_cor_df)
```

#### Test Assumptions for Multiple linear regression
- Normality of target variable (checked!: histogram & qqplot good!)
- Linearity (checked!: corrplot good!)
- Independence (checked! But not good...)
- No multicollinearity (checked! VIF test good!)
- Homoscedasticity (checked! Breusch-Pagan test. But not good...)
- No influential outliers (checked! cock's distance. But not good...)

#### Multiple linear regression model Evaluation
(From Textbook)
5. Model Evaluation 
- R-squared: R-squared is a measure of the proportion of variation in the dependent variable that is explained by the independent variables in the model. Higher R-squared values indicate a better fit of the model to the data. However, R-squared alone may not provide a complete picture of the model's performance.
- Significance of coefficient
- Residual analysis: Residual analysis involves examining the residuals (the differences between the observed and predicted values of the dependent variable) to assess the fit of the model. A good model should have residuals that are normally distributed with a mean of 0 and constant variance (homoscedasticity). Residual plots can help identify outliers and non-linear relationships in the data.
- F-test: The F-test is a statistical test that compares the fit of the full model (with all independent variables) to a reduced model (with fewer independent variables). A significant F-test indicates that the full model provides a better fit to the data than the reduced model.
- if comparing models: anova
- if comparing models of different # of variables: use other criteria such as Adj R2, BIC, AIC, Marlow's Cp.
- Cross-validation: Cross-validation involves splitting the data into training and testing sets to evaluate the performance of the model on new data. Cross-validation can help assess the generalizability of the model to new data and identify potential issues with overfitting.
6 .Model selection: feature selection, interaction terms, etc. 
Steps 5 and 6 might repeat to many times in search of a good model. Other times might need to restart from 2 to try out different model structures. 
```{r}
summary(depression_model)
summary(mhlth_model)
```

##### No Auto-colinearity (independence)
The Durbin-Watson test is a statistical test that evaluates whether the residuals (errors) in a regression model are independent of each other.
This will output the Durbin-Watson test statistic, the degrees of freedom, and the p-value. The null hypothesis of the Durbin-Watson test is that there is no autocorrelation (independence) in the residuals. A p-value less than 0.05 suggests evidence of autocorrelation, while a p-value greater than 0.05 suggests no evidence of autocorrelation.
```{r}
# The Durbin-Watson test
library(lmtest)
depression_dwtest <- dwtest(depression_model)
mhlth_dwtest <- dwtest(mhlth_model)

depression_dwtest
mhlth_dwtest
```

##### Homoscedasticity
Breusch-Pagan test: The Breusch-Pagan test is a statistical test for homoscedasticity that tests whether the variance of the residuals is constant across all levels of the independent variables. This will output the test statistic and the p-value for the Breusch-Pagan test. The null hypothesis of the test is that the variance of the residuals is constant (homoscedasticity). A p-value less than 0.05 suggests evidence of heteroscedasticity.
```{r}
# Breusch-Pagan test
bptest(depression_model)
bptest(mhlth_model)
```

##### No influential outliers
Cook's distance: Cook's distance is a statistical measure of the influence of each observation on the regression coefficients. An observation with a Cook's distance greater than 1 is often considered to be influential.

Leverage values: Leverage values are another measure of the influence of each observation on the regression coefficients. An observation with a leverage value greater than 2p/n, where p is the number of independent variables and n is the sample size, is often considered to be influential.
```{r}
# Cook's distance and Leverage

# depression
depression_influential <- influence.measures(depression_model)
depression_infmat <- depression_influential$infmat
sum(depression_infmat[, 'cook.d'] > 1)
sum(depression_infmat[, 'hat'] > 2 * 7 / length(depression_infmat[, 'hat']))

# mhlth
mhlth_influential <- influence.measures(mhlth_model)
mhlth_infmat <- mhlth_influential$infmat
sum(mhlth_infmat[, 'cook.d'] > 1)
sum(mhlth_infmat[, 'hat'] > 2 * 7 / length(depression_infmat[, 'hat']))
```


### Regression Tree Model
- Splitting the data: The dataset is split into training and testing sets to evaluate the model's performance.
- Building the tree: The regression tree is built using a recursive partitioning algorithm, which splits the data into subsets based on the independent variables that best predict the dependent variable.
- Pruning the tree: The tree is pruned to reduce overfitting and improve generalization to new data.
```{r}
#regression tree model for depression
# control_params1 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
# 
# depression.tree <- rpart(DEPRESSION ~ LPA + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + URBANIZED_AREA_POP_CEN_2010 + pct_URBANIZED_AREA_POP_CEN_2010, data = depression_cor_df, method="class", control = control_params1)
# 
# plot(depression.tree, uniform=TRUE, main="Depression Rate Tree")
# text(depression.tree, use.n=TRUE, all=TRUE, cex=.8)
# 
# p <- prp(depression.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
# 
# 
# # regression tree model for poor mental health
# control_params2 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
# 
# mhlth.tree <- rpart(MHLTH ~ LPA + SLEEP + MT_never_married + MT_now_married + MT_Separated + EA_Less_than_high_school_graduate + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed, data = mhlth_cor_df, method = "class", control = control_params2)
# 
# plot(mhlth.tree, uniform=TRUE, main="Poor Mental Health Rate Tree")
# text(mhlth.tree, use.n=TRUE, all=TRUE, cex=.8)
# 
# p <- prp(mhlth.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
  

# fancy tree plots
# fancy depression tree
depression.tree.2 <- rpart(DEPRESSION ~ LPA + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + URBANIZED_AREA_POP_CEN_2010 + pct_URBANIZED_AREA_POP_CEN_2010, data = depression_cor_df)

fancyRpartPlot(depression.tree.2, main = "Depression Rate Tree")

# fancy mental health tree
mhlth.tree.2 <- rpart(MHLTH ~ LPA + SLEEP + MT_never_married + MT_now_married + MT_Separated + EA_Less_than_high_school_graduate + EA_High_school_graduate + EA_Bachelors_degree + EA_Graduate_or_professional_degree + MI_Estimate + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed, data = mhlth_cor_df, control = rpart.control(maxdepth = ))

fancyRpartPlot(mhlth.tree.2, main = "Poor Mental Health Rate Tree")

# pruning trees
# pruned depression tree
# pruned.depression.tree.2 <- prune(depression.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.depression.tree.2)
# 
# # pruned mental health tree
# pruned.mhlth.tree.2 <- prune(mhlth.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.mhlth.tree.2)
```

#### Regression Tree Model Assumptions
- Regression tree assumptions:
- Independence of observations: Each observation must be independent of all other observations.
- Linearity: There must be a linear relationship between the dependent variable and the independent variables.
- Homoscedasticity: The variance of the residuals should be constant across all levels of the dependent variable.
- Normality: The residuals should be normally distributed.

#### Regression Tree Evaluation
- Evaluating the model: The model is evaluated using various metrics such as mean squared error (MSE), root mean squared error (RMSE), and R-squared.
- Validating the model: The model is validated using the testing set to assess its ability to generalize to new data.
- Tuning the model: The model is tuned by adjusting the hyperparameters to improve its performance.
- Interpreting the model: The regression tree is interpreted to gain insights into the relationships between the independent variables and the dependent variable.
```{r}
```


## Result
Need edited

## Discussion
Need edited

### Limitations
Need edited

### Further Research
Need edited

## Conclusion
Need edited

## References
Need edited
# Reference links to datasets:
https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-Census-Tract-D/cwsq-ngmh
https://gisgeography.com/us-county-map/
https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data/
https://data.census.gov/
https://www.anl.gov/dis/county-economic-impact-index
https://www.census.gov/programs-surveys/community-resilience-estimates/data/datasets.html
https://github.com/NREL/hsds-examples

